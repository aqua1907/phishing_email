{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read data\n",
    "Read the data from CSV and apply some basic pre-processing (remove non-ascii characters, convert our target variable to an integer label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.stem import SnowballStemmer\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Function to clean the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    #text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read data\n",
    "Read the data from CSV and applying preprocessing steps and then converting text and labels into different matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [] \n",
    "# labels = []\n",
    "# with codecs.open(r'data/all_email_data.csv', encoding='utf-8') as f:\n",
    "#     reader = csv.reader(f, delimiter=',')\n",
    "#     header = next(reader)\n",
    "#     for values in reader:\n",
    "#         docs.append(text_to_wordlist(values[0]))\n",
    "#         labels.append((values[1]))\n",
    "\n",
    "data = pd.read_csv(r\"data/all_email_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(text_to_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naturally irresistible your corporate identity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the stock trading gunslinger fanny is merrill ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unbelievable new homes made easy im wanting to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 color printing special request additional in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do not have money get software cds from here !...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25695</th>\n",
       "      <td>preferred non - smoker + + just what the doct...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25696</th>\n",
       "      <td>dear subscriber + + if i could show you a way ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25697</th>\n",
       "      <td>mid - summer customer appreciation sale ! + +...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25698</th>\n",
       "      <td>attn : sir madan + + strictly confidential + +...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25699</th>\n",
       "      <td>mv 00001 317e78fa8ee2f54cd4890fdc09ba8176 0000...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25700 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  spam\n",
       "0      naturally irresistible your corporate identity...     1\n",
       "1      the stock trading gunslinger fanny is merrill ...     1\n",
       "2      unbelievable new homes made easy im wanting to...     1\n",
       "3      4 color printing special request additional in...     1\n",
       "4      do not have money get software cds from here !...     1\n",
       "...                                                  ...   ...\n",
       "25695   preferred non - smoker + + just what the doct...     1\n",
       "25696  dear subscriber + + if i could show you a way ...     1\n",
       "25697   mid - summer customer appreciation sale ! + +...     1\n",
       "25698  attn : sir madan + + strictly confidential + +...     1\n",
       "25699  mv 00001 317e78fa8ee2f54cd4890fdc09ba8176 0000...     1\n",
       "\n",
       "[25700 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Encoding label class as 1 if class is 'spam' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [1 if x == 'spam' else 0 for x in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Tokenize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. calculation vocabulary size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125024"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Making sequence of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125024 unique tokens\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "print('Found %s unique tokens' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Padding sequence with maximum length of 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "padded_docs = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "labels = np.array(data['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Loading pre-trained dictionary of word embeddings that translates each word into a 100 dimensional vector.\n",
    "More info on the project that created this dataset https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open(r'data/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    #print(values)\n",
    "    word = values[0]\n",
    "    #print(word)\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'of': 4,\n",
       " 'a': 5,\n",
       " 'you': 6,\n",
       " 'for': 7,\n",
       " 'in': 8,\n",
       " 'i': 9,\n",
       " 'is': 10,\n",
       " 'ect': 11,\n",
       " 'on': 12,\n",
       " 'this': 13,\n",
       " 'that': 14,\n",
       " 'it': 15,\n",
       " 'be': 16,\n",
       " 'enron': 17,\n",
       " 'com': 18,\n",
       " 'your': 19,\n",
       " 'with': 20,\n",
       " 'have': 21,\n",
       " 'not': 22,\n",
       " 'we': 23,\n",
       " 'will': 24,\n",
       " 'are': 25,\n",
       " 'from': 26,\n",
       " 'at': 27,\n",
       " 'hou': 28,\n",
       " 'as': 29,\n",
       " 'http': 30,\n",
       " 'or': 31,\n",
       " 'by': 32,\n",
       " 'if': 33,\n",
       " 's': 34,\n",
       " '1': 35,\n",
       " '2000': 36,\n",
       " 'am': 37,\n",
       " 'can': 38,\n",
       " 'please': 39,\n",
       " '2': 40,\n",
       " 'me': 41,\n",
       " 'our': 42,\n",
       " 'subject': 43,\n",
       " 'all': 44,\n",
       " 'do': 45,\n",
       " 'an': 46,\n",
       " 'www': 47,\n",
       " '3': 48,\n",
       " 'my': 49,\n",
       " 'would': 50,\n",
       " 'was': 51,\n",
       " '0': 52,\n",
       " 'but': 53,\n",
       " 'has': 54,\n",
       " 'any': 55,\n",
       " 'cc': 56,\n",
       " '00': 57,\n",
       " '10': 58,\n",
       " 'vince': 59,\n",
       " 'pm': 60,\n",
       " 're': 61,\n",
       " 'get': 62,\n",
       " 'so': 63,\n",
       " 'no': 64,\n",
       " 'new': 65,\n",
       " 'one': 66,\n",
       " 'net': 67,\n",
       " 'more': 68,\n",
       " 'they': 69,\n",
       " 'there': 70,\n",
       " 'up': 71,\n",
       " 'email': 72,\n",
       " 'time': 73,\n",
       " 'e': 74,\n",
       " '2001': 75,\n",
       " 'know': 76,\n",
       " 'out': 77,\n",
       " 'list': 78,\n",
       " '5': 79,\n",
       " 'about': 80,\n",
       " 'gas': 81,\n",
       " 'thanks': 82,\n",
       " '4': 83,\n",
       " 'what': 84,\n",
       " '000': 85,\n",
       " 'information': 86,\n",
       " 'may': 87,\n",
       " 'deal': 88,\n",
       " 'mv': 89,\n",
       " 'just': 90,\n",
       " 'which': 91,\n",
       " 'like': 92,\n",
       " 'need': 93,\n",
       " '01': 94,\n",
       " 'd': 95,\n",
       " 'been': 96,\n",
       " 'now': 97,\n",
       " 'only': 98,\n",
       " 'us': 99,\n",
       " 'here': 100,\n",
       " 'message': 101,\n",
       " 't': 102,\n",
       " 'some': 103,\n",
       " 'should': 104,\n",
       " 'he': 105,\n",
       " '11': 106,\n",
       " 'these': 107,\n",
       " 'corp': 108,\n",
       " 'font': 109,\n",
       " 'p': 110,\n",
       " 'see': 111,\n",
       " 'also': 112,\n",
       " '12': 113,\n",
       " 'when': 114,\n",
       " 'their': 115,\n",
       " 'company': 116,\n",
       " 'mail': 117,\n",
       " 'other': 118,\n",
       " 'meter': 119,\n",
       " 'let': 120,\n",
       " '20': 121,\n",
       " 'use': 122,\n",
       " 'free': 123,\n",
       " 'hpl': 124,\n",
       " 'b': 125,\n",
       " 'm': 126,\n",
       " 'how': 127,\n",
       " 'who': 128,\n",
       " '7': 129,\n",
       " 'group': 130,\n",
       " 'make': 131,\n",
       " 'into': 132,\n",
       " 'br': 133,\n",
       " 'business': 134,\n",
       " 'j': 135,\n",
       " 'day': 136,\n",
       " 'forwarded': 137,\n",
       " '6': 138,\n",
       " '30': 139,\n",
       " 'price': 140,\n",
       " 'linux': 141,\n",
       " 'could': 142,\n",
       " '02': 143,\n",
       " 'them': 144,\n",
       " 'people': 145,\n",
       " '8': 146,\n",
       " 'energy': 147,\n",
       " '03': 148,\n",
       " '09': 149,\n",
       " 'over': 150,\n",
       " '2002': 151,\n",
       " 'call': 152,\n",
       " 'its': 153,\n",
       " 'work': 154,\n",
       " 'want': 155,\n",
       " 'daren': 156,\n",
       " '04': 157,\n",
       " 'c': 158,\n",
       " 'than': 159,\n",
       " 'l': 160,\n",
       " '9': 161,\n",
       " 'had': 162,\n",
       " 'sent': 163,\n",
       " '08': 164,\n",
       " 'his': 165,\n",
       " 'first': 166,\n",
       " 'then': 167,\n",
       " 'very': 168,\n",
       " 'well': 169,\n",
       " 'send': 170,\n",
       " '3d': 171,\n",
       " 'th': 172,\n",
       " 'were': 173,\n",
       " '15': 174,\n",
       " 'click': 175,\n",
       " '05': 176,\n",
       " 'research': 177,\n",
       " 'back': 178,\n",
       " 'x': 179,\n",
       " 'attached': 180,\n",
       " 'think': 181,\n",
       " 'go': 182,\n",
       " 'does': 183,\n",
       " 'help': 184,\n",
       " 'name': 185,\n",
       " 'r': 186,\n",
       " 'report': 187,\n",
       " 'html': 188,\n",
       " 'take': 189,\n",
       " 'contact': 190,\n",
       " 'best': 191,\n",
       " '99': 192,\n",
       " 'good': 193,\n",
       " 'system': 194,\n",
       " 'users': 195,\n",
       " 'number': 196,\n",
       " 'order': 197,\n",
       " 'file': 198,\n",
       " 'risk': 199,\n",
       " 'forward': 200,\n",
       " 'following': 201,\n",
       " 'through': 202,\n",
       " 'money': 203,\n",
       " 'ie': 204,\n",
       " 'today': 205,\n",
       " 'after': 206,\n",
       " 'power': 207,\n",
       " 'year': 208,\n",
       " 'change': 209,\n",
       " 'date': 210,\n",
       " 'way': 211,\n",
       " 'next': 212,\n",
       " 'o': 213,\n",
       " 'u': 214,\n",
       " 'questions': 215,\n",
       " 'mmbtu': 216,\n",
       " 'week': 217,\n",
       " 'market': 218,\n",
       " 'address': 219,\n",
       " 'most': 220,\n",
       " 'lists': 221,\n",
       " 'month': 222,\n",
       " 'because': 223,\n",
       " 'two': 224,\n",
       " 'below': 225,\n",
       " 'org': 226,\n",
       " 'much': 227,\n",
       " 'nbsp': 228,\n",
       " 'many': 229,\n",
       " 'last': 230,\n",
       " 'management': 231,\n",
       " 'jkaminski': 232,\n",
       " 'look': 233,\n",
       " 'listinfo': 234,\n",
       " 'before': 235,\n",
       " 'available': 236,\n",
       " '25': 237,\n",
       " 'said': 238,\n",
       " 'phone': 239,\n",
       " 'meeting': 240,\n",
       " 'houston': 241,\n",
       " '07': 242,\n",
       " 'internet': 243,\n",
       " '06': 244,\n",
       " 'did': 245,\n",
       " 'find': 246,\n",
       " 'home': 247,\n",
       " 'per': 248,\n",
       " 'days': 249,\n",
       " 'original': 250,\n",
       " 'each': 251,\n",
       " 'where': 252,\n",
       " 'line': 253,\n",
       " 'world': 254,\n",
       " 'set': 255,\n",
       " 'contract': 256,\n",
       " 'software': 257,\n",
       " 'within': 258,\n",
       " '16': 259,\n",
       " '100': 260,\n",
       " 'td': 261,\n",
       " '19': 262,\n",
       " 'same': 263,\n",
       " 'farmer': 264,\n",
       " 'kaminski': 265,\n",
       " 'even': 266,\n",
       " 'program': 267,\n",
       " '24': 268,\n",
       " 'size': 269,\n",
       " 'such': 270,\n",
       " '18': 271,\n",
       " 'service': 272,\n",
       " 'web': 273,\n",
       " '50': 274,\n",
       " 'those': 275,\n",
       " 'give': 276,\n",
       " 'site': 277,\n",
       " '21': 278,\n",
       " 'thank': 279,\n",
       " 'still': 280,\n",
       " 'being': 281,\n",
       " 'data': 282,\n",
       " 'using': 283,\n",
       " '14': 284,\n",
       " 'mailing': 285,\n",
       " '713': 286,\n",
       " 'robert': 287,\n",
       " 'right': 288,\n",
       " 'made': 289,\n",
       " 'texas': 290,\n",
       " 'mailman': 291,\n",
       " '31': 292,\n",
       " '23': 293,\n",
       " 'based': 294,\n",
       " 'rpm': 295,\n",
       " 'wrote': 296,\n",
       " 'again': 297,\n",
       " 'xls': 298,\n",
       " 'development': 299,\n",
       " 'office': 300,\n",
       " 'services': 301,\n",
       " 'going': 302,\n",
       " 'fax': 303,\n",
       " 'news': 304,\n",
       " 'she': 305,\n",
       " 'volume': 306,\n",
       " 'off': 307,\n",
       " 'john': 308,\n",
       " 'receive': 309,\n",
       " 'looking': 310,\n",
       " 'under': 311,\n",
       " 'since': 312,\n",
       " '22': 313,\n",
       " 'sure': 314,\n",
       " 'online': 315,\n",
       " 'edu': 316,\n",
       " '17': 317,\n",
       " 'end': 318,\n",
       " 'great': 319,\n",
       " 'check': 320,\n",
       " 'regards': 321,\n",
       " 'years': 322,\n",
       " 'both': 323,\n",
       " 'request': 324,\n",
       " 'july': 325,\n",
       " 'co': 326,\n",
       " 'him': 327,\n",
       " '13': 328,\n",
       " 'w': 329,\n",
       " '28': 330,\n",
       " 'inc': 331,\n",
       " 'support': 332,\n",
       " 'don': 333,\n",
       " 'future': 334,\n",
       " 'received': 335,\n",
       " 'visit': 336,\n",
       " 'few': 337,\n",
       " '27': 338,\n",
       " 'ena': 339,\n",
       " 'high': 340,\n",
       " 'deals': 341,\n",
       " 'f': 342,\n",
       " 'n': 343,\n",
       " 'long': 344,\n",
       " 'access': 345,\n",
       " 'shirley': 346,\n",
       " 'offer': 347,\n",
       " 'without': 348,\n",
       " 'sitara': 349,\n",
       " 'used': 350,\n",
       " 'her': 351,\n",
       " 'while': 352,\n",
       " 'credit': 353,\n",
       " 'hi': 354,\n",
       " '29': 355,\n",
       " 'april': 356,\n",
       " 'spamassassin': 357,\n",
       " 'state': 358,\n",
       " 'every': 359,\n",
       " 'changes': 360,\n",
       " 'color': 361,\n",
       " 'down': 362,\n",
       " 'working': 363,\n",
       " 'ca': 364,\n",
       " 'process': 365,\n",
       " 'friday': 366,\n",
       " 'project': 367,\n",
       " 'come': 368,\n",
       " 'info': 369,\n",
       " 'production': 370,\n",
       " 'products': 371,\n",
       " 'problem': 372,\n",
       " 'product': 373,\n",
       " 'account': 374,\n",
       " 'sales': 375,\n",
       " 'k': 376,\n",
       " 'spam': 377,\n",
       " 'provide': 378,\n",
       " 'team': 379,\n",
       " 'volumes': 380,\n",
       " 'communications': 381,\n",
       " 'real': 382,\n",
       " 'financial': 383,\n",
       " 'nom': 384,\n",
       " 'trading': 385,\n",
       " 'march': 386,\n",
       " '26': 387,\n",
       " 'might': 388,\n",
       " 'razor': 389,\n",
       " 'link': 390,\n",
       " 'daily': 391,\n",
       " 'h': 392,\n",
       " 'prices': 393,\n",
       " 'model': 394,\n",
       " 'bob': 395,\n",
       " 'companies': 396,\n",
       " 'start': 397,\n",
       " 'million': 398,\n",
       " 'talk': 399,\n",
       " 'part': 400,\n",
       " 'g': 401,\n",
       " 'another': 402,\n",
       " 'security': 403,\n",
       " 'own': 404,\n",
       " 'however': 405,\n",
       " 'able': 406,\n",
       " 'less': 407,\n",
       " 'really': 408,\n",
       " 'form': 409,\n",
       " 'between': 410,\n",
       " 'flow': 411,\n",
       " 'point': 412,\n",
       " 'pec': 413,\n",
       " 'too': 414,\n",
       " 'center': 415,\n",
       " 'll': 416,\n",
       " 'america': 417,\n",
       " 'face': 418,\n",
       " 'better': 419,\n",
       " 'ilug': 420,\n",
       " 'once': 421,\n",
       " 'current': 422,\n",
       " 'must': 423,\n",
       " 'why': 424,\n",
       " 'got': 425,\n",
       " 'life': 426,\n",
       " 'above': 427,\n",
       " 'something': 428,\n",
       " 'investment': 429,\n",
       " 'possible': 430,\n",
       " 'reply': 431,\n",
       " 'currently': 432,\n",
       " 'gary': 433,\n",
       " 'keep': 434,\n",
       " 'monday': 435,\n",
       " 'statements': 436,\n",
       " 'case': 437,\n",
       " 'place': 438,\n",
       " 'issue': 439,\n",
       " 'version': 440,\n",
       " 'total': 441,\n",
       " 'due': 442,\n",
       " 'id': 443,\n",
       " 'yahoo': 444,\n",
       " 'effective': 445,\n",
       " 'mark': 446,\n",
       " '40': 447,\n",
       " 'conference': 448,\n",
       " 'put': 449,\n",
       " 'interested': 450,\n",
       " 'type': 451,\n",
       " 'review': 452,\n",
       " 'june': 453,\n",
       " 'read': 454,\n",
       " 'cost': 455,\n",
       " 'hope': 456,\n",
       " 'copy': 457,\n",
       " 'stock': 458,\n",
       " 'north': 459,\n",
       " 'sourceforge': 460,\n",
       " 'marketing': 461,\n",
       " 'david': 462,\n",
       " 'until': 463,\n",
       " 'global': 464,\n",
       " 'interest': 465,\n",
       " 'needs': 466,\n",
       " 'note': 467,\n",
       " 'say': 468,\n",
       " 'code': 469,\n",
       " 'august': 470,\n",
       " 'full': 471,\n",
       " 'already': 472,\n",
       " 'special': 473,\n",
       " 'try': 474,\n",
       " 'oil': 475,\n",
       " 'mr': 476,\n",
       " 'purchase': 477,\n",
       " 'include': 478,\n",
       " 'dear': 479,\n",
       " 'remove': 480,\n",
       " 'website': 481,\n",
       " 'options': 482,\n",
       " 'uk': 483,\n",
       " 'content': 484,\n",
       " 'things': 485,\n",
       " 'further': 486,\n",
       " 'january': 487,\n",
       " 'term': 488,\n",
       " 'save': 489,\n",
       " 'done': 490,\n",
       " 'value': 491,\n",
       " 'during': 492,\n",
       " 'v': 493,\n",
       " 'old': 494,\n",
       " 'mailto': 495,\n",
       " 'buy': 496,\n",
       " 'sale': 497,\n",
       " 'university': 498,\n",
       " 'including': 499,\n",
       " 'user': 500,\n",
       " 'agreement': 501,\n",
       " 'hours': 502,\n",
       " 'rate': 503,\n",
       " 'etc': 504,\n",
       " 'ticket': 505,\n",
       " 'tr': 506,\n",
       " 'someone': 507,\n",
       " 'steve': 508,\n",
       " 'technology': 509,\n",
       " 'different': 510,\n",
       " 'never': 511,\n",
       " 'pay': 512,\n",
       " 'months': 513,\n",
       " 'mary': 514,\n",
       " 'anyone': 515,\n",
       " 'around': 516,\n",
       " 'ees': 517,\n",
       " 'text': 518,\n",
       " 'width': 519,\n",
       " 'michael': 520,\n",
       " 'url': 521,\n",
       " 'run': 522,\n",
       " 'government': 523,\n",
       " 'feel': 524,\n",
       " 'getting': 525,\n",
       " 'soon': 526,\n",
       " 'thursday': 527,\n",
       " 'page': 528,\n",
       " 'top': 529,\n",
       " 'tuesday': 530,\n",
       " 'person': 531,\n",
       " 'de': 532,\n",
       " 'times': 533,\n",
       " 'network': 534,\n",
       " 'show': 535,\n",
       " 'regarding': 536,\n",
       " 'details': 537,\n",
       " 'na': 538,\n",
       " 'finance': 539,\n",
       " 'server': 540,\n",
       " 'london': 541,\n",
       " 'three': 542,\n",
       " 'freshrpms': 543,\n",
       " 'believe': 544,\n",
       " 'anything': 545,\n",
       " 'american': 546,\n",
       " 'september': 547,\n",
       " 'shall': 548,\n",
       " 'book': 549,\n",
       " 'works': 550,\n",
       " 'additional': 551,\n",
       " 'windows': 552,\n",
       " 'course': 553,\n",
       " 'unsubscribe': 554,\n",
       " 'crenshaw': 555,\n",
       " 'industry': 556,\n",
       " 'via': 557,\n",
       " 'cd': 558,\n",
       " 'smith': 559,\n",
       " 'option': 560,\n",
       " 'december': 561,\n",
       " 'found': 562,\n",
       " 'position': 563,\n",
       " 'results': 564,\n",
       " 'states': 565,\n",
       " 'computer': 566,\n",
       " 'kevin': 567,\n",
       " 'fw': 568,\n",
       " 'october': 569,\n",
       " 'trade': 570,\n",
       " '500': 571,\n",
       " 'either': 572,\n",
       " 'opportunity': 573,\n",
       " 'performance': 574,\n",
       " 'international': 575,\n",
       " 'issues': 576,\n",
       " '45': 577,\n",
       " 'open': 578,\n",
       " 'meet': 579,\n",
       " 'plant': 580,\n",
       " 'nomination': 581,\n",
       " 'george': 582,\n",
       " 'customer': 583,\n",
       " 'interview': 584,\n",
       " 'href': 585,\n",
       " 'stop': 586,\n",
       " 'stinson': 587,\n",
       " 'doc': 588,\n",
       " 'resources': 589,\n",
       " 'systems': 590,\n",
       " '60': 591,\n",
       " 'fork': 592,\n",
       " 'morning': 593,\n",
       " 'markets': 594,\n",
       " 'resume': 595,\n",
       " '44': 596,\n",
       " 'easy': 597,\n",
       " 'plan': 598,\n",
       " 'julie': 599,\n",
       " 'tom': 600,\n",
       " 'public': 601,\n",
       " 'little': 602,\n",
       " 'delivery': 603,\n",
       " 'yet': 604,\n",
       " 'doing': 605,\n",
       " 'php': 606,\n",
       " 'index': 607,\n",
       " 've': 608,\n",
       " 'probably': 609,\n",
       " 'president': 610,\n",
       " 'always': 611,\n",
       " 'add': 612,\n",
       " 'manager': 613,\n",
       " 'update': 614,\n",
       " 'messages': 615,\n",
       " 'customers': 616,\n",
       " 'thing': 617,\n",
       " 'files': 618,\n",
       " 'mike': 619,\n",
       " 'move': 620,\n",
       " 'legal': 621,\n",
       " 'having': 622,\n",
       " 'united': 623,\n",
       " 'wish': 624,\n",
       " '95': 625,\n",
       " 'subscription': 626,\n",
       " 'yes': 627,\n",
       " 'desk': 628,\n",
       " 'align': 629,\n",
       " 'important': 630,\n",
       " 'family': 631,\n",
       " 'pricing': 632,\n",
       " 'several': 633,\n",
       " 'exmh': 634,\n",
       " 'discuss': 635,\n",
       " 'sf': 636,\n",
       " 'transaction': 637,\n",
       " 'continue': 638,\n",
       " 'presentation': 639,\n",
       " 'ask': 640,\n",
       " 'aol': 641,\n",
       " 'problems': 642,\n",
       " 'numbers': 643,\n",
       " 'fact': 644,\n",
       " 'un': 645,\n",
       " 'https': 646,\n",
       " 'department': 647,\n",
       " 'big': 648,\n",
       " 'given': 649,\n",
       " 'weeks': 650,\n",
       " 'schedule': 651,\n",
       " 'error': 652,\n",
       " 'november': 653,\n",
       " 'control': 654,\n",
       " 'box': 655,\n",
       " 'director': 656,\n",
       " 'follow': 657,\n",
       " 'wednesday': 658,\n",
       " 'ever': 659,\n",
       " 'school': 660,\n",
       " 'lot': 661,\n",
       " 'non': 662,\n",
       " 'tell': 663,\n",
       " 'rates': 664,\n",
       " 'melissa': 665,\n",
       " 'else': 666,\n",
       " 'cash': 667,\n",
       " 'called': 668,\n",
       " 'notice': 669,\n",
       " 'experience': 670,\n",
       " 'ok': 671,\n",
       " 'create': 672,\n",
       " 'natural': 673,\n",
       " 'release': 674,\n",
       " 'february': 675,\n",
       " 'making': 676,\n",
       " 'low': 677,\n",
       " 'tx': 678,\n",
       " 'area': 679,\n",
       " 'needed': 680,\n",
       " 'complete': 681,\n",
       " 'vance': 682,\n",
       " 'card': 683,\n",
       " 'job': 684,\n",
       " 'together': 685,\n",
       " 'act': 686,\n",
       " 'past': 687,\n",
       " 'lon': 688,\n",
       " 'second': 689,\n",
       " 'teco': 690,\n",
       " 'love': 691,\n",
       " 'newsletter': 692,\n",
       " 'removed': 693,\n",
       " 'rice': 694,\n",
       " 'professional': 695,\n",
       " 'microsoft': 696,\n",
       " 'perl': 697,\n",
       " 'hello': 698,\n",
       " 'source': 699,\n",
       " 'offers': 700,\n",
       " 'thought': 701,\n",
       " 'general': 702,\n",
       " 'increase': 703,\n",
       " 'later': 704,\n",
       " 'happy': 705,\n",
       " 'level': 706,\n",
       " 'height': 707,\n",
       " 'sorry': 708,\n",
       " 'personal': 709,\n",
       " 'comments': 710,\n",
       " 'transport': 711,\n",
       " 'question': 712,\n",
       " 'dollars': 713,\n",
       " 'search': 714,\n",
       " 'sell': 715,\n",
       " 'fyi': 716,\n",
       " 'actual': 717,\n",
       " 'hard': 718,\n",
       " 'though': 719,\n",
       " 'irish': 720,\n",
       " 'country': 721,\n",
       " 'adobe': 722,\n",
       " 'everyone': 723,\n",
       " 'return': 724,\n",
       " 'feedback': 725,\n",
       " 'key': 726,\n",
       " 'securities': 727,\n",
       " 'pat': 728,\n",
       " 'kind': 729,\n",
       " 'letter': 730,\n",
       " '90': 731,\n",
       " 'small': 732,\n",
       " 'bank': 733,\n",
       " 'ami': 734,\n",
       " 'trying': 735,\n",
       " 'section': 736,\n",
       " 'others': 737,\n",
       " 'left': 738,\n",
       " 'wanted': 739,\n",
       " 'response': 740,\n",
       " 'actually': 741,\n",
       " 'package': 742,\n",
       " 'susan': 743,\n",
       " 'dr': 744,\n",
       " 'least': 745,\n",
       " 'pipeline': 746,\n",
       " 'application': 747,\n",
       " 'short': 748,\n",
       " 'summer': 749,\n",
       " 'feb': 750,\n",
       " 'operations': 751,\n",
       " 'remember': 752,\n",
       " 'example': 753,\n",
       " 'paid': 754,\n",
       " 'contracts': 755,\n",
       " 'terms': 756,\n",
       " 'reports': 757,\n",
       " 'james': 758,\n",
       " '2004': 759,\n",
       " 'against': 760,\n",
       " 'write': 761,\n",
       " 'st': 762,\n",
       " 'limited': 763,\n",
       " 'htm': 764,\n",
       " 'jackie': 765,\n",
       " 'supply': 766,\n",
       " 'asked': 767,\n",
       " 'groups': 768,\n",
       " 'understand': 769,\n",
       " 'related': 770,\n",
       " 'xent': 771,\n",
       " 'party': 772,\n",
       " 'everything': 773,\n",
       " 'provided': 774,\n",
       " 'coming': 775,\n",
       " 'required': 776,\n",
       " 'redhat': 777,\n",
       " '32': 778,\n",
       " 'period': 779,\n",
       " 'large': 780,\n",
       " 'seems': 781,\n",
       " 'paper': 782,\n",
       " 'says': 783,\n",
       " '98': 784,\n",
       " 'demand': 785,\n",
       " 'become': 786,\n",
       " 'created': 787,\n",
       " 'storage': 788,\n",
       " 'added': 789,\n",
       " 'projects': 790,\n",
       " 'y': 791,\n",
       " 'capital': 792,\n",
       " 'enough': 793,\n",
       " 'orders': 794,\n",
       " 'whether': 795,\n",
       " 'city': 796,\n",
       " 'man': 797,\n",
       " 'aug': 798,\n",
       " 'corporation': 799,\n",
       " 'nothing': 800,\n",
       " 'members': 801,\n",
       " 'amount': 802,\n",
       " 'prior': 803,\n",
       " 'major': 804,\n",
       " 'brian': 805,\n",
       " 'view': 806,\n",
       " 'maintainer': 807,\n",
       " 'lisa': 808,\n",
       " 'analysis': 809,\n",
       " 'welcome': 810,\n",
       " '35': 811,\n",
       " '80': 812,\n",
       " 'jeff': 813,\n",
       " 'cannot': 814,\n",
       " 'tomorrow': 815,\n",
       " 'tap': 816,\n",
       " 'listmaster': 817,\n",
       " 'away': 818,\n",
       " 'spot': 819,\n",
       " 'field': 820,\n",
       " 'running': 821,\n",
       " 'bill': 822,\n",
       " 'potential': 823,\n",
       " 'upon': 824,\n",
       " 'quality': 825,\n",
       " 'chris': 826,\n",
       " 'arial': 827,\n",
       " 'aimee': 828,\n",
       " 'activity': 829,\n",
       " 'join': 830,\n",
       " 'build': 831,\n",
       " 'instead': 832,\n",
       " 'jan': 833,\n",
       " 'article': 834,\n",
       " 'taylor': 835,\n",
       " 'radio': 836,\n",
       " 'simply': 837,\n",
       " 'weather': 838,\n",
       " 'told': 839,\n",
       " 'changed': 840,\n",
       " 'bit': 841,\n",
       " 'allow': 842,\n",
       " 'download': 843,\n",
       " 'young': 844,\n",
       " 'fuel': 845,\n",
       " 'california': 846,\n",
       " 'firm': 847,\n",
       " 'plus': 848,\n",
       " 'ready': 849,\n",
       " 'latest': 850,\n",
       " 'local': 851,\n",
       " 'far': 852,\n",
       " 'direct': 853,\n",
       " 'mind': 854,\n",
       " 'addresses': 855,\n",
       " 'sponsored': 856,\n",
       " 'jones': 857,\n",
       " 'immediately': 858,\n",
       " 'martin': 859,\n",
       " 'utilities': 860,\n",
       " 'noms': 861,\n",
       " 'ms': 862,\n",
       " 'asp': 863,\n",
       " 'success': 864,\n",
       " 'grant': 865,\n",
       " '36': 866,\n",
       " 'bring': 867,\n",
       " 'database': 868,\n",
       " 'private': 869,\n",
       " 'maybe': 870,\n",
       " '55': 871,\n",
       " '1999': 872,\n",
       " 'a3': 873,\n",
       " 'sites': 874,\n",
       " '853': 875,\n",
       " 'drive': 876,\n",
       " 'employees': 877,\n",
       " 'head': 878,\n",
       " 'scheduled': 879,\n",
       " 'included': 880,\n",
       " 'correct': 881,\n",
       " 'table': 882,\n",
       " 'tickets': 883,\n",
       " 'necessary': 884,\n",
       " 'yourself': 885,\n",
       " 'pills': 886,\n",
       " 'operating': 887,\n",
       " 'class': 888,\n",
       " 'sending': 889,\n",
       " 'nd': 890,\n",
       " '200': 891,\n",
       " 'ago': 892,\n",
       " 'action': 893,\n",
       " 'test': 894,\n",
       " 'clear': 895,\n",
       " 'lee': 896,\n",
       " 'reason': 897,\n",
       " 'tenaska': 898,\n",
       " 'training': 899,\n",
       " 'hsc': 900,\n",
       " 'word': 901,\n",
       " 'sincerely': 902,\n",
       " 'share': 903,\n",
       " 'jim': 904,\n",
       " 'path': 905,\n",
       " '33': 906,\n",
       " 'style': 907,\n",
       " 'gibner': 908,\n",
       " 'opportunities': 909,\n",
       " 'lloyd': 910,\n",
       " '34': 911,\n",
       " 'idea': 912,\n",
       " 'media': 913,\n",
       " 'emails': 914,\n",
       " 'stuff': 915,\n",
       " 'fast': 916,\n",
       " 'late': 917,\n",
       " 'enronxgate': 918,\n",
       " 'thinkgeek': 919,\n",
       " 'fee': 920,\n",
       " 'border': 921,\n",
       " 'cotten': 922,\n",
       " 'log': 923,\n",
       " 'richard': 924,\n",
       " '59': 925,\n",
       " 'specific': 926,\n",
       " 'night': 927,\n",
       " 'transfer': 928,\n",
       " 'actuals': 929,\n",
       " 'taking': 930,\n",
       " 'longer': 931,\n",
       " 'national': 932,\n",
       " 'friends': 933,\n",
       " 'chokshi': 934,\n",
       " 'advice': 935,\n",
       " 'q': 936,\n",
       " 'var': 937,\n",
       " 'simple': 938,\n",
       " '800': 939,\n",
       " 'event': 940,\n",
       " 'begin': 941,\n",
       " 'matter': 942,\n",
       " 'duke': 943,\n",
       " 'corporate': 944,\n",
       " 'receiving': 945,\n",
       " 'side': 946,\n",
       " '48': 947,\n",
       " 'events': 948,\n",
       " 'early': 949,\n",
       " 'policy': 950,\n",
       " 'listed': 951,\n",
       " 'mentioned': 952,\n",
       " 'close': 953,\n",
       " 'recent': 954,\n",
       " 'body': 955,\n",
       " 'commercial': 956,\n",
       " 'whole': 957,\n",
       " 'learn': 958,\n",
       " 'rather': 959,\n",
       " 'nice': 960,\n",
       " 'exchange': 961,\n",
       " 'insurance': 962,\n",
       " 'status': 963,\n",
       " 'quite': 964,\n",
       " 'board': 965,\n",
       " 'room': 966,\n",
       " 'basis': 967,\n",
       " '39': 968,\n",
       " 'stanford': 969,\n",
       " 'appreciate': 970,\n",
       " 'apt': 971,\n",
       " 'standard': 972,\n",
       " 'programs': 973,\n",
       " 'ac': 974,\n",
       " 'design': 975,\n",
       " 'answer': 976,\n",
       " '2005': 977,\n",
       " 'live': 978,\n",
       " 'agree': 979,\n",
       " 'makes': 980,\n",
       " 'names': 981,\n",
       " '49': 982,\n",
       " 'paul': 983,\n",
       " 'megan': 984,\n",
       " 'along': 985,\n",
       " 'approval': 986,\n",
       " 'directly': 987,\n",
       " 'dec': 988,\n",
       " 'charge': 989,\n",
       " 'means': 990,\n",
       " 'addition': 991,\n",
       " 'howard': 992,\n",
       " 'b5': 993,\n",
       " 'location': 994,\n",
       " 'tax': 995,\n",
       " '46': 996,\n",
       " 'canada': 997,\n",
       " 'final': 998,\n",
       " 'hplc': 999,\n",
       " 'third': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. we only need the subset of these 400,000 words that appear in our docs.So , we create a weight matrix for words in training docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125025, 100)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RNN():\n",
    "#     inputs = Input(name='inputs',shape=[max_len])\n",
    "#     layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)(inputs)\n",
    "#     layer = LSTM(64)(layer)\n",
    "#     layer = Dense(256,name='FC1')(layer)\n",
    "#     layer = Activation('relu')(layer)\n",
    "#     layer = Dropout(0.5)(layer)\n",
    "#     layer = Dense(1,name='out_layer')(layer)\n",
    "#     layer = Activation('sigmoid')(layer)\n",
    "#     model = Model(inputs=inputs,outputs=layer)\n",
    "#     return model\n",
    "\n",
    "# def rnn_2():\n",
    "#     inputs = Input(name='inputs',shape=[max_len])\n",
    "#     embd = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)(inputs)\n",
    "#     X = LSTM(128, return_sequences=True)(embd)\n",
    "#     # Add dropout with a probability of 0.5\n",
    "#     X = Dropout(0.5)(X)\n",
    "#     # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "#     X = LSTM(128, return_sequences=False)(X)\n",
    "#     X = Dropout(0.5)(X)\n",
    "#     # Propagate X through a Dense layer with 5 units and add softmax activation\n",
    "#     X = Dense(1, activation=\"sigmoid\")(X)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=X)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          12502500  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 299, 64)           12864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 149, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 149, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 148, 128)          16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 74, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 74, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 73, 256)           65792     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 36, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 36, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9217      \n",
      "=================================================================\n",
      "Total params: 12,606,885\n",
      "Trainable params: 12,606,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
    "model.add(Conv1D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(128, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(256, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model = rnn_2()\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20560,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1.,\n",
    "                1: 3.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taring and Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16448 samples, validate on 4112 samples\n",
      "Epoch 1/50\n",
      "16448/16448 [==============================] - 85s 5ms/sample - loss: 0.4738 - accuracy: 0.8631 - val_loss: 0.2648 - val_accuracy: 0.9596\n",
      "Epoch 2/50\n",
      "16448/16448 [==============================] - 85s 5ms/sample - loss: 0.1440 - accuracy: 0.9669 - val_loss: 0.1850 - val_accuracy: 0.9553\n",
      "Epoch 3/50\n",
      "16448/16448 [==============================] - 80s 5ms/sample - loss: 0.0608 - accuracy: 0.9871 - val_loss: 0.4180 - val_accuracy: 0.9664\n",
      "Epoch 4/50\n",
      "16448/16448 [==============================] - 86s 5ms/sample - loss: 0.0221 - accuracy: 0.9951 - val_loss: 0.3270 - val_accuracy: 0.9779\n",
      "Epoch 5/50\n",
      "16448/16448 [==============================] - 82s 5ms/sample - loss: 0.0355 - accuracy: 0.9941 - val_loss: 0.3445 - val_accuracy: 0.9771\n",
      "Epoch 6/50\n",
      "16448/16448 [==============================] - 80s 5ms/sample - loss: 0.0155 - accuracy: 0.9970 - val_loss: 0.3922 - val_accuracy: 0.9776\n",
      "Epoch 7/50\n",
      "16448/16448 [==============================] - 78s 5ms/sample - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.3901 - val_accuracy: 0.9557\n",
      "Accuracy: 96.089494\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop],\n",
    "         class_weight=class_weight)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam = \"\"\"Dear Counsel,\n",
    "We acknowledge, with thanks, your email and the instructions contained therein. Our representative will collect the\n",
    "hard copy documents.\n",
    "We are proceeding to action the instructions and will keep you updated.\n",
    "With best regards,\n",
    "SF\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear counsel + we acknowledge with thanks your email and the instructions contained therein our representative will collect the + hard copy documents + we are proceeding to action the instructions and will keep you updated + with best regards + sf'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_spam = text_to_wordlist(not_spam)\n",
    "not_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam = tokenizer.texts_to_sequences([not_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam = pad_sequences(not_spam, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8551485]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = \"\"\"Subject: congratulations hpshum you \\' ve won !  congratulations !  official  notification  hpshum @ hotmail . com  you have been specially selected to register for a  \n",
    "florida / bahamas vacation !  you will enjoy :  8 days / 7 nights of lst class accomodations  valid for up to 4 travelers  rental car with  unlimited mileage  adult casino cruise  \n",
    "great florida attractions !  much much more . . .  click here !  ( limited availability )  to no longer receive this or any other offer from us , click here to unsubscribe .  \n",
    "[ bjk 9 ^ \" : } h & * tgobk 5 nkiys 5 ]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = text_to_wordlist(spam)\n",
    "spam = tokenizer.texts_to_sequences([spam])\n",
    "spam = pad_sequences(spam, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99990416]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam = \"\"\"Subject: congratulations  vince ,  congratulations on your promotion to managing director . you certainly deserve  it .  zhiyong\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_spam = text_to_wordlist(not_spam)\n",
    "not_spam = tokenizer.texts_to_sequences([not_spam])\n",
    "not_spam = pad_sequences(not_spam, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00056683]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = \"\"\"Hello!\n",
    "\n",
    "I am a hacker who has access to your operating system.\n",
    "I also have full access to your account.\n",
    "\n",
    "I've been watching you for a few months now.\n",
    "The fact is that you were infected with malware through an adult site that you visited.\n",
    "\n",
    "If you are not familiar with this, I will explain.\n",
    "Trojan Virus gives me full access and control over a computer or other device.\n",
    "This means that I can see everything on your screen, turn on the camera and microphone, but you do not know about it.\n",
    "\n",
    "I also have access to all your contacts and all your correspondence.\n",
    "\n",
    "Why your antivirus did not detect malware?\n",
    "Answer: My malware uses the driver, I update its signatures every 4 hours so that your antivirus is silent.\n",
    "\n",
    "I made a video showing how you satisfy yourself in the left half of the screen, and in the right half you see the video that you watched.\n",
    "With one click of the mouse, I can send this video to all your emails and contacts on social networks.\n",
    "I can also post access to all your e-mail correspondence and messengers that you use.\n",
    "\n",
    "If you want to prevent this,\n",
    "transfer the amount of $500 to my bitcoin address (if you do not know how to do this, write to Google: \"Buy Bitcoin\").\n",
    "\n",
    "My bitcoin address (BTC Wallet) is:  15gyQqNaV7n6befX6gTvn1LHR8GQBPUc2A\n",
    "\n",
    "After receiving the payment, I will delete the video and you will never hear me again.\n",
    "I give you 50 hours (more than 2 days) to pay.\n",
    "I have a notice reading this letter, and the timer will work when you see this letter.\n",
    "\n",
    "Filing a complaint somewhere does not make sense because this email cannot be tracked like my bitcoin address.\n",
    "I do not make any mistakes.\n",
    "\n",
    "If I find that you have shared this message with someone else, the video will be immediately distributed.\n",
    "\n",
    "Best regards!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = text_to_wordlist(spam)\n",
    "spam = tokenizer.texts_to_sequences([spam])\n",
    "spam = pad_sequences(spam, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9981748]]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(spam)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = \"\"\"Dear User,\n",
    "Courtesy Notice from Admin Team\n",
    "You have reached the storage limit for your Mailbox and database server.\n",
    "You will be blocked from sending and receiving new messages. If your email is not verified within 48 hours.\n",
    "Please click BELOW to verify and access the e-mail restore.\n",
    "\n",
    "CLICK HERE\n",
    "Thanks,\n",
    "WINDOWS LIVE TEAM\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.606051]]\n"
     ]
    }
   ],
   "source": [
    "spam = text_to_wordlist(spam)\n",
    "spam = tokenizer.texts_to_sequences([spam])\n",
    "spam = pad_sequences(spam, maxlen=max_len, padding='post')\n",
    "result = model.predict(spam)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606051"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'model/spam_detector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/classifier.pkl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(tokenizer, r'model/classifier.pkl', compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
